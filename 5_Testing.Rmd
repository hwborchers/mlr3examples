---
title: "Resampling and Benchmarking"
author: "Hans W. Borchers"
date: "July 2020"
output:
  html_document:
    css: "mlr3.css"
    keep_md: true
    toc: true
    toc_float:
      collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE}
library(mlr3verse, quietly = TRUE)
library(mlr3learners.c50)
```

## Data

We will look at how the 'mlr3' package family supports testing of learners. We will use the same data as before.
```{r}
Glass <- RWeka::read.arff("glass.arff")
dim(Glass)
```

The task, as defined before, will also be needed.
```{r}
mytask <- TaskClassif$new(id = "glass",
                          backend = Glass, target = "Type")
```


## Resampling

*Resampling* is a method to create training and test splits of the data. 'mlr3' supports cross-validation, leave-one-out, bootstrap, and hold-out methods.

To see all resampling methods available in the base 'mlr3' family, display the `mlr_resamplings` dictionary:

```{r}
as.data.table(mlr_resamplings)
```

We will try out the k-fold cross-validation, here given as "cv", doing it k times (with $k^2 models to generate). In the literature the recommended value is $k=10$. We we do a simple 10-fold cross-validation to keep running times small.


### Resampling strategy

First we select the resampling strategy as "cross validation". The default is *ten-fold* and can be changed with the `folds` argument. The drawback is that other resampling methods will have different option arguments. 

```{r}
# resample = mlr_resamplings$get("cv")  # rsmp
resample = rsmp("cv")  # rsmp("cv", folds = 10)
resample
```

As learner, let's again take the classical CART algorithm for classification.

```{r}
learner_cart <- lrn("classif.rpart")
# learner_cart
```


### Cross validation

Now we apply the resampling procedure to get a better estimate of the accuracy. No need to define training and testing data, resampling will do this for us.
```{r}
rresult = resample(task = mytask, learner_cart, resample)
```

```{r}
rresult$score(msr("classif.acc"))
```

```{r}
# result$aggregate()
round(rresult$aggregate(msrs(c("classif.acc", "classif.ce"))), 2)
```


## Benchmarking

Create a benchmark design:
```{r}
design = benchmark_grid(
  tasks = mytask,
  learners = list(lrn("classif.rpart"), lrn("classif.ranger"),
                  lrn("classif.C5.0"), lrn("classif.kknn")),
  resamplings = rsmp("cv", folds = 5)
)
print(design)
```

Start the benchmarking process:
```{r}
bmark = benchmark(design)
```

Define a list of measures and then aggregate the measures:
```{r}
measures = list(msr("classif.acc"), msr("time_train"))
bmark$aggregate(measures)
```

## Extending the example

```r
learners = c("classif.featureless", "classif.rpart", "classif.ranger", "classif.kknn")
learners = lapply(learners, lrn,
  predict_type = "prob", predict_sets = c("train", "test"))
```
